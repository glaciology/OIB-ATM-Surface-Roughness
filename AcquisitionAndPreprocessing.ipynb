{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Acquisition and Pre-Processing Script for Operation IceBridge Airborne Topographic Mapper L2 Icessn Elevation, Slope, and Roughness, v.2 Data\n",
    "\n",
    "This script automates requests for data and compiles and filters data to within the ice extent. \n",
    "Data are filtered to within the GrIS 2004 ice extent, determined by the USGS EROS National Atlas project (http://cinergi.sdsc.edu/geoportal/rest/metadata/item/6b9cbb294e8d49268c17f396d476113b/html)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download command -- run in the directory where you want data saved! \n",
    "\n",
    "wget --http-user=<username> --http-password=<password> --load-cookies\n",
    "~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies\n",
    "--no-check-certificate --auth-no-challenge=on -r --reject \"index.html*\"\n",
    "--reject \"*.xml\" -q -np -e robots=off\n",
    "https://n5eil01u.ecs.nsidc.org/ICEBRIDGE/ILATM2.002/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os \n",
    "import glob \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import geopandas as gpd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day(fname): \n",
    "    \"\"\" \n",
    "    Process each day's downloaded .csv files and combine them into a single output file. \n",
    "    Function extracts date from folder name, adds 'Year', 'Month', and 'Day' columns, and \n",
    "    saved the processed data in a structured directory. \n",
    "\n",
    "    Parameters: \n",
    "    - fname (str): Path to folder containing .csv files for a specific day. \n",
    "      Folder name must follow 'YYYYMMDD' format. \n",
    "\n",
    "    Returns: \n",
    "    - Creates directories in the structure: '../../oib_ATM_test/{year}/{month}/'\n",
    "    - Writes combined .csv file in created directories with the format: '{day}.csv'\n",
    "    \"\"\"\n",
    "\n",
    "    files=glob.glob(fname+'/*.csv') # Get all .csv files in folder \n",
    "    if len(files)==0: # If no files are found, exit \n",
    "        return \n",
    "    \n",
    "    date=fname.split('/')[-1].split('.') # Extract date from folder name\n",
    "    format_dif=0\n",
    "    if date[0]==['2019']:\n",
    "        format_dif=1\n",
    "\n",
    "    os.makedirs('../../oib_ATM_test/{}/{}'.format(date[0],date[1]), exist_ok=True) # Make output directory for the year and month\n",
    "    \n",
    "    f=open(files[0],'r') # Open read files\n",
    "    f_new=open('../../oib_ATM_test/{}/{}/{}.csv'.format(date[0],date[1],date[2]),'w') # Open output file for writing\n",
    "    \n",
    "    for i in files: \n",
    "        f=open(i,'r')\n",
    "        if first==False: # Skip headers for subsequent files\n",
    "            for j in range(10+format_dif):\n",
    "                f.readline()\n",
    "        else: # Include headers for the first file \n",
    "            for j in range(9+format_dif):\n",
    "                f.readline()\n",
    "            f_new.write(f.readline()[2:-1]+', Year, Month, Day\\n') # Add year, month, and day columns \n",
    "            first=False \n",
    "\n",
    "        for j in f: # Add rows and append year, month, and day\n",
    "            f_new.write(j[:-1]+',{},{},{}\\n'.format(date[0],date[1],date[2])),\n",
    "    \n",
    "    f.close() # Close read files \n",
    "    f_new.close() # Close write files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=glob.glob('./*') # Get all .csvs \n",
    "for i in files: # Process all .csvs \n",
    "    day(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=glob.glob('./*/*/*.csv') # Get processed .csvs \n",
    "for i in files: \n",
    "    name = i.split('.')[1].split('/') \n",
    "    newname = '{}{}'.format(name[-2], name[-1]) # Get month and day from file path \n",
    "    os.rename(i,'./{}/{}/{}.csv'.format(name[1],name[2],newname)) # Rename files to MMDD within year folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to concatenate each month and filter to ice extent \n",
    "\n",
    "def make_years(base_path, output_file, ice_extent_shp): \n",
    "    \"\"\"\n",
    "    Process monthly .csv files, combining them into yearly datasets, \n",
    "    and filter data points to include only those within the Greenland ice extent.\n",
    "\n",
    "    Parameters:\n",
    "    - base_path (str): Path to the base directory containing the data organized in YY/MM folders.\n",
    "    - output_file (str): Path to the output directory where yearly filtered data will be saved.\n",
    "    - ice_extent_shp (str): Path to the shapefile defining the Greenland ice extent.\n",
    "    \n",
    "    Returns:\n",
    "    - Processed yearly .csv files saved in `/path/to/output/<year>/<year>.csv`.\n",
    "    \"\"\"\n",
    "    ice_extent = gpd.read_file(ice_extent_shp) # Load ice extent shapefile\n",
    "    ice_extent = ice_extent.to_crs(epsg=3413) # Set .shp CRS to polar stereographic \n",
    "    month_folders = glob.glob(os.path.join(base_path, '*/*')) # Get all YY/MM folders within base path \n",
    "    yearly_data = {} # Dict to hold yearly files  \n",
    "\n",
    "    for month_folder in month_folders: \n",
    "        parts = month_folder.split(os.sep) # Get YY/MM from folder structure \n",
    "        year = parts[-2] \n",
    "        month = parts[-1] \n",
    "\n",
    "        if year not in yearly_data: # Initialize DataFrame for yera if not already created \n",
    "            yearly_data[year] = pd.DataFrame()\n",
    "\n",
    "        csv_files = glob.glob(os.path.join(month_folder, '*.csv')) # Get all .csv files in the month folder \n",
    "        month_df = pd.DataFrame()\n",
    "\n",
    "        for file in csv_files: # Iterate through each .csv in month folder \n",
    "            df = pd.read_csv(file) # Read file into a DataFrame \n",
    "            month_df = pd.concat([month_df, df], ignore_index=True) # Append data to the month's DataFrame \n",
    "\n",
    "        yearly_data[year] = pd.concat([yearly_data[year], month_df], ignore_index=True) # Append months to yearly DataFrame \n",
    "    \n",
    "    for year, data in yearly_data.items(): # Process each year's data \n",
    "        gdf = gpd.GeoDataFrame( # Convert data to GeoDataFrame \n",
    "            data, \n",
    "            geometry=gpd.points_from_xy(data['Longitude(deg)'],data['Latitude(deg)']), \n",
    "            crs='epsg:4326'\n",
    "        ) \n",
    "\n",
    "        gdf = gdf.to_crs(epsg=3413) # Reproject to polar stereographic \n",
    "        gdf['Easting'] = gdf.geometry.x # Add Easting and Northing columns \n",
    "        gdf['Northing'] = gdf.geometry.y \n",
    "        gdf_in = gpd.sjoin(gdf, ice_extent, op='within') # Perform spatila join to filter points within ice extent \n",
    "        year_output_dir = os.path.join(output_dir, year) # Define output directory for current year \n",
    "        os.makedirs(year_output_dir, exist_ok=True)\n",
    "        output_file = os.path.join(year_output_dir, f'{year}.csv') # Define output file path \n",
    "        gdf_in.drop(columns='geometry', index=False) # Remove geometry column\n",
    "        gdf_in.to_csv(output_file, index=False) # Save filtered data to .csv file \n",
    "        print(f\"Saved data for {year} to {output_file}\") # Print message confirming the save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_extent_shp = './greenland_ice_extent/greenland_ice_extent.shp' # Path to ice extent .shp\n",
    "base_path = './' # Path to ILATM2 base directory \n",
    "output_dir = '/yearly_data' # Path to output directory\n",
    "os.makedirs(output_dir, exist_ok=True) # Make output directory if it doesn't already exist. \n",
    "make_years(base_path, output_dir, ice_extent_shp) # Process and filter yearly data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
